Unicode:

assigns codepoints to over 100000 characters from many languages
a unicode character spans 21 bits (21 digits)
shares data values with ascii
Unicde 'a' is 97 base 10 == U+0061 = 110 0001 base 2, which is the same as ascii
unicode just defines the CODES for each caracters, but there are still
different ways to actually encode unicode
UTF-8 - Unicode Transformation FOrmat
- varialbe length unicode representation, backwards compatible with ASCII

Patterns: so the actual unicode will be the x's and y's
then you PAD them accordinng to how many bits and therefore what pattern it is
1 byte: xxx yyyy (7 bit binary number)
        0xxx yyyy

2 byte: xxxxx yyyyyy 11 bit binary number
        110x xxxx 10yy yyyy

3 byte: xxxx yyyyyy zzzzzz (16 bit binary number)
    1110 xxxx 10yy yyyy 10zz zzzz

4 byte: xxx yyyyyy zzzzzz wwwwww (21 bit binary number)
    1111 0xxx 10yy yyyy 10zz zzzz 10ww wwww

Ex: snowflake image is U+2744 = 10 0111 0100 0100
there are 14 bits --> 3 bytes

so pad with 0s for now
0010 0111 0100 0100
now divide into groups of 6 and add appropriatly
xxxx yyyyyy zzzzzz
1110 0010 10011101 10000100
Steps According to Kash:
- we wil lbe given binary representation of it
- count number of #s given
- <=7 is 1 bytes
- 7 to 11 is 2 bytes
11 to 16 is 3 bytes
16 to 21 is 4 bytes
each byte is 4 bits
answer will have number of bytes * 8
so 4 bytes will have 32 numbers

divide the numbers i have into groups of 6 (or just 7 if there is only one byte)

then if it is 3 bytes, you add n 1s to front and a zero
and for all the other groups of six you add 10
so for three byte yo uad to front
1110 xxxx 10 yy yyyy 10 zz zzzz

(so for each of the rest you split into 6 and add a 10 on to the front of it )
